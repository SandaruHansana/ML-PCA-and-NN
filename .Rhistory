outlierIndices <- vector("list", length = ncol(df))
for (i in seq_along(df)) {
Q <- quantile(df[[i]], c(0.25, 0.75), na.rm = TRUE)
IQR <- Q[2] - Q[1]
lowerBound <- Q[1] - 1.5 * IQR
upperBound <- Q[2] + 1.5 * IQR
outlierIndices[[i]] <- which(df[[i]] < lowerBound | df[[i]] > upperBound)
}
# Combine outlier indices
allOutlierIndices <- unique(unlist(outlierIndices))
# Remove outliers
dfNoOutliers <- df[-allOutlierIndices, ]
# Check if any outliers were removed
if (length(allOutlierIndices) > 0) {
cat("Removed", length(allOutlierIndices), "outliers.\n")
} else {
cat("No outliers were found.\n")
}
# Scaling the data
dfScaled <- scale(dfNoOutliers)
# Print the outliers chart
boxplot(df)
boxplot(dfScaled)
# Check for missing values, NaNs, or Infs
if (any(is.na(dfScaled)) || any(!is.finite(dfScaled))) {
# Handle missing values, NaNs, or Infs
# One simple method is to remove them
dfScaled <- dfScaled[complete.cases(dfScaled), ]
cat("Removed missing values, NaNs, or Infs.\n")
}
# Print the cleaned and scaled dataset
print(dfScaled)
class(dfScaled)
# NbClust
nbResults <- NbClust(dfScaled, min.nc = 2, max.nc = 10, method = "kmeans", index = "all")
# Elbow Method
wcss <- numeric(10)
for (i in 1:10) {
kmeansFit <- kmeans(dfScaled, centers = i)
wcss[i] <- kmeansFit$tot.withinss
}
# Plotting the Elbow Method
plot(1:10, wcss, type = "b", xlab = "Number of Clusters", ylab = "Within-Cluster Sum of Squares (WCSS)", main = "Elbow Method")
# Silhouette score
silhouetteScore <- function(k){
km <- kmeans(dfScaled, centers = k, nstart=25)
ss <- silhouette(km$cluster, dist(dfScaled))
mean(ss[, 3])
}
k <- 2:10
avgSil <- sapply(k, silhouetteScore)
plot(k, avgSil, type='b', xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)
# Gap statistics
fvizNbclust(dfScaled, kmeans, method = 'gap_stat')
# Load required libraries
library(readxl)
library(tidyverse)
library(cluster)
library(NbClust)
library(datasets)
library(factoextra)
library(cluster)
# Read the CSV file
df <- read_excel("D:/IIT/Machine Learning/CW/CW/Whitewine_v6.xlsx")
# Remove the "quality" feature
df <- df[, -which(names(df) == "quality")]
# Identify potential outliers using Tukey's method
outlierIndices <- vector("list", length = ncol(df))
for (i in seq_along(df)) {
Q <- quantile(df[[i]], c(0.25, 0.75), na.rm = TRUE)
IQR <- Q[2] - Q[1]
lowerBound <- Q[1] - 1.5 * IQR
upperBound <- Q[2] + 1.5 * IQR
outlierIndices[[i]] <- which(df[[i]] < lowerBound | df[[i]] > upperBound)
}
# Combine outlier indices
allOutlierIndices <- unique(unlist(outlierIndices))
# Remove outliers
dfNoOutliers <- df[-allOutlierIndices, ]
# Check if any outliers were removed
if (length(allOutlierIndices) > 0) {
cat("Removed", length(allOutlierIndices), "outliers.\n")
} else {
cat("No outliers were found.\n")
}
# Scaling the data
dfScaled <- scale(dfNoOutliers)
# Print the outliers chart
boxplot(df)
boxplot(dfScaled)
# Check for missing values, NaNs, or Infs
if (any(is.na(dfScaled)) || any(!is.finite(dfScaled))) {
# Handle missing values, NaNs, or Infs
# One simple method is to remove them
dfScaled <- dfScaled[complete.cases(dfScaled), ]
cat("Removed missing values, NaNs, or Infs.\n")
}
# Print the cleaned and scaled dataset
print(dfScaled)
class(dfScaled)
# NbClust
nbResults <- NbClust(dfScaled, min.nc = 2, max.nc = 10, method = "kmeans", index = "all")
# Elbow Method
wcss <- numeric(10)
for (i in 1:10) {
kmeansFit <- kmeans(dfScaled, centers = i)
wcss[i] <- kmeansFit$tot.withinss
}
# Plotting the Elbow Method
plot(1:10, wcss, type = "b", xlab = "Number of Clusters", ylab = "Within-Cluster Sum of Squares (WCSS)", main = "Elbow Method")
# Silhouette score
silhouetteScore <- function(k){
km <- kmeans(dfScaled, centers = k, nstart=25)
ss <- silhouette(km$cluster, dist(dfScaled))
mean(ss[, 3])
}
k <- 2:10
avgSil <- sapply(k, silhouetteScore)
plot(k, avgSil, type='b', xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)
# Gap statistics
#fvizNbclust(dfScaled, kmeans, method = 'gap_stat')
# k-means clustering
k <- 2
kmeansWine <- kmeans(dfScaled, centers = k, nstart = 10)
print(kmeansWine)
# Extract cluster centers
centers <- kmeansWine$centers
clusterAssignments <- kmeansWine$cluster
# Calculate the total sum of squares (TSS)
TSS <- sum(apply(dfScaled, 1, function(x) sum((x - mean(dfScaled))^2)))
# Calculate the between-cluster sum of squares (BSS)
BSS <- sum(apply(centers, 1, function(c) sum((c - mean(dfScaled))^2))) * k
# Calculate the within-cluster sum of squares (WSS)
WSS <- sum(kmeansWine$withinss)
# Calculate the ratio of BSS to TSS
BSS_TSS_ratio <- BSS / TSS
# Print the calculated indices
cat("Total Sum of Squares (TSS):", TSS, "\n")
cat("Between-Cluster Sum of Squares (BSS):", BSS, "\n")
cat("Within-Cluster Sum of Squares (WSS):", WSS, "\n")
cat("Ratio of BSS to TSS:", BSS_TSS_ratio, "\n")
# Get cluster centers
centers <- kmeansWine$centers
print("Cluster Centers:")
print(centers)
# Calculate silhouette scores
silhouetteScores <- silhouette(kmeansWine$cluster, dist(dfScaled))
# Plot silhouette plot
plot(silhouetteScores, col = kmeansWine$cluster, border = NA)
# Add average silhouette width to the plot
abline(h = mean(silhouetteScores[, "sil_width"]), lty = 2)
# Average silhouette width score
avgSilhouetteWidth <- mean(silhouetteScores[, "sil_width"])
print(avgSilhouetteWidth)
# Load required libraries
library(readxl)
library(tidyverse)
library(cluster)
library(NbClust)
library(datasets)
library(factoextra)
library(cluster)
library(fpc)
# Read the CSV file
dfChangeRate <- read_excel("D:/IIT/Machine Learning/CW/CW/Whitewine_v6.xlsx")
# Remove the "quality" feature
dfChangeRate <- dfChangeRate[, -which(names(dfChangeRate) == "quality")]
# Identify potential outliers using Tukey's method
outlierIndicesChangeRate <- vector("list", length = ncol(dfChangeRate))
for (i in seq_along(dfChangeRate)) {
QChangeRate <- quantile(dfChangeRate[[i]], c(0.25, 0.75), na.rm = TRUE)
IQRChangeRate <- QChangeRate[2] - QChangeRate[1]
lowerBoundChangeRate <- QChangeRate[1] - 1.5 * IQRChangeRate
upperBoundChangeRate <- QChangeRate[2] + 1.5 * IQRChangeRate
outlierIndicesChangeRate[[i]] <- which(dfChangeRate[[i]] < lowerBoundChangeRate | dfChangeRate[[i]] > upperBoundChangeRate)
}
# Combine outlier indices
allOutlierIndicesChangeRate <- unique(unlist(outlierIndicesChangeRate))
# Remove outliers
dfNoOutliersChangeRate <- dfChangeRate[-allOutlierIndicesChangeRate, ]
# Scaling the data
dfScaledChangeRate <- scale(dfNoOutliersChangeRate)
# Perform PCA
pcaResultChangeRate <- prcomp(dfScaledChangeRate, scale = TRUE)
# Eigenvalues and eigenvectors from PCA
eigenvaluesPcaChangeRate <- pcaResultChangeRate$sdev^2
eigenvectorsPcaChangeRate <- pcaResultChangeRate$rotation
# Print eigenvalues, eigenvectors, and summary
print(eigenvaluesPcaChangeRate)
print(eigenvectorsPcaChangeRate)
summary(pcaResultChangeRate)
# Cumulative variance explained by each PC
cumulativeVariance <- cumsum(pcaResultChangeRate$sdev^2 / sum(pcaResultChangeRate$sdev^2) * 100)
print(cumulativeVariance)
# Find the index of the PC that provides at least 85% cumulative variance
numPcs <- which(cumulativeVariance >= 85)[1]
print(numPcs)
# Select the PCs and Create a new dataframe with selected principal components
selectedPcs <- pcaResultChangeRate$x[, 1:numPcs]
newDf <- as.data.frame(selectedPcs)
print(head(newDf))
# NbClust
nbResults <- NbClust(newDf, min.nc = 2, max.nc = 10, method = "kmeans", index = "all")
# Elbow Method
wcss <- numeric(10)
for (i in 1:10) {
kmeansFit <- kmeans(newDf, centers = i)
wcss[i] <- kmeansFit$tot.withinss
}
# Plotting the Elbow Method
plot(1:10, wcss, type = "b", xlab = "Number of Clusters", ylab = "Within-Cluster Sum of Squares (WCSS)", main = "Elbow Method")
# Silhouette score
silhouetteScore <- function(k){
km <- kmeans(newDf, centers = k, nstart=25)
ss <- silhouette(km$cluster, dist(newDf))
mean(ss[, 3])
}
k <- 2:10
avgSil <- sapply(k, silhouetteScore)
plot(k, avgSil, type='b', xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)
# Gap statistics
#fvizNbclust(newDf, kmeans, method = 'gap_stat')
# k-means clustering
k <- 2
kmeansWine <- kmeans(newDf, centers = k, nstart = 10)
print(kmeansWine)
# Extract cluster centers
centers <- kmeansWine$centers
clusterAssignments <- kmeansWine$cluster
# Calculate the total sum of squares (TSS)
TSS <- sum(apply(newDf, 1, function(x) sum((x - mean(newDf))^2)))
# Calculate the between-cluster sum of squares (BSS)
BSS <- sum(apply(centers, 1, function(c) sum((c - mean(newDf))^2))) * k
# Calculate the within-cluster sum of squares (WSS)
WSS <- sum(kmeansWine$withinss)
# Calculate the ratio of BSS to TSS
BSS_TSS_ratio <- BSS / TSS
# Print the calculated indices
cat("Total Sum of Squares (TSS):", TSS, "\n")
cat("Between-Cluster Sum of Squares (BSS):", BSS, "\n")
cat("Within-Cluster Sum of Squares (WSS):", WSS, "\n")
cat("Ratio of BSS to TSS:", BSS_TSS_ratio, "\n")
# Get cluster centers
centers <- kmeansWine$centers
print("Cluster Centers:")
print(centers)
# Calculate silhouette scores
silhouetteScores <- silhouette(kmeansWine$cluster, dist(newDf))
# Plot silhouette plot
plot(silhouetteScores, col = kmeansWine$cluster, border = NA)
# Add average silhouette width to the plot
abline(h = mean(silhouetteScores[, "sil_width"]), lty = 2)
# Average silhouette width score
avgSilhouetteWidth <- mean(silhouetteScores[, "sil_width"])
print(avgSilhouetteWidth)
# Calculate Calinski-Harabasz index using cluster.stats
calinski_harabasz <- cluster.stats(dist(newDf), kmeansWine$cluster)$ch
print("Calinski-Harabasz Index:")
print(calinski_harabasz)
# Load required libraries
library(readxl)
library(tidyverse)
library(cluster)
library(NbClust)
library(datasets)
library(factoextra)
library(cluster)
library(fpc)
# Read the CSV file
dfChangeRate <- read_excel("D:/IIT/Machine Learning/CW/CW/Whitewine_v6.xlsx")
# Remove the "quality" feature
dfChangeRate <- dfChangeRate[, -which(names(dfChangeRate) == "quality")]
# Identify potential outliers using Tukey's method
outlierIndicesChangeRate <- vector("list", length = ncol(dfChangeRate))
for (i in seq_along(dfChangeRate)) {
QChangeRate <- quantile(dfChangeRate[[i]], c(0.25, 0.75), na.rm = TRUE)
IQRChangeRate <- QChangeRate[2] - QChangeRate[1]
lowerBoundChangeRate <- QChangeRate[1] - 1.5 * IQRChangeRate
upperBoundChangeRate <- QChangeRate[2] + 1.5 * IQRChangeRate
outlierIndicesChangeRate[[i]] <- which(dfChangeRate[[i]] < lowerBoundChangeRate | dfChangeRate[[i]] > upperBoundChangeRate)
}
# Combine outlier indices
allOutlierIndicesChangeRate <- unique(unlist(outlierIndicesChangeRate))
# Remove outliers
dfNoOutliersChangeRate <- dfChangeRate[-allOutlierIndicesChangeRate, ]
# Scaling the data
dfScaledChangeRate <- scale(dfNoOutliersChangeRate)
# Perform PCA
pcaResultChangeRate <- prcomp(dfScaledChangeRate, scale = TRUE)
# Eigenvalues and eigenvectors from PCA
eigenvaluesPcaChangeRate <- pcaResultChangeRate$sdev^2
eigenvectorsPcaChangeRate <- pcaResultChangeRate$rotation
# Print eigenvalues, eigenvectors, and summary
print(eigenvaluesPcaChangeRate)
print(eigenvectorsPcaChangeRate)
summary(pcaResultChangeRate)
# Cumulative variance explained by each PC
cumulativeVariance <- cumsum(pcaResultChangeRate$sdev^2 / sum(pcaResultChangeRate$sdev^2) * 100)
print(cumulativeVariance)
# Find the index of the PC that provides at least 85% cumulative variance
numPcs <- which(cumulativeVariance >= 85)[1]
print(numPcs)
# Select the PCs and Create a new dataframe with selected principal components
selectedPcs <- pcaResultChangeRate$x[, 1:numPcs]
newDf <- as.data.frame(selectedPcs)
print(head(newDf))
boxplot(dfChangeRate)
boxplot(newDf)
# NbClust
#nbResults <- NbClust(newDf, min.nc = 2, max.nc = 10, method = "kmeans", index = "all")
# Elbow Method
wcss <- numeric(10)
for (i in 1:10) {
kmeansFit <- kmeans(newDf, centers = i)
wcss[i] <- kmeansFit$tot.withinss
}
# Plotting the Elbow Method
plot(1:10, wcss, type = "b", xlab = "Number of Clusters", ylab = "Within-Cluster Sum of Squares (WCSS)", main = "Elbow Method")
# Silhouette score
silhouetteScore <- function(k){
km <- kmeans(newDf, centers = k, nstart=25)
ss <- silhouette(km$cluster, dist(newDf))
mean(ss[, 3])
}
k <- 2:10
avgSil <- sapply(k, silhouetteScore)
plot(k, avgSil, type='b', xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)
# Gap statistics
#fvizNbclust(newDf, kmeans, method = 'gap_stat')
# k-means clustering
k <- 2
kmeansWine <- kmeans(newDf, centers = k, nstart = 10)
print(kmeansWine)
# Extract cluster centers
centers <- kmeansWine$centers
clusterAssignments <- kmeansWine$cluster
# Calculate the total sum of squares (TSS)
TSS <- sum(apply(newDf, 1, function(x) sum((x - mean(newDf))^2)))
# Calculate the between-cluster sum of squares (BSS)
BSS <- sum(apply(centers, 1, function(c) sum((c - mean(newDf))^2))) * k
# Calculate the within-cluster sum of squares (WSS)
WSS <- sum(kmeansWine$withinss)
# Calculate the ratio of BSS to TSS
BSS_TSS_ratio <- BSS / TSS
# Print the calculated indices
cat("Total Sum of Squares (TSS):", TSS, "\n")
cat("Between-Cluster Sum of Squares (BSS):", BSS, "\n")
cat("Within-Cluster Sum of Squares (WSS):", WSS, "\n")
cat("Ratio of BSS to TSS:", BSS_TSS_ratio, "\n")
# Get cluster centers
centers <- kmeansWine$centers
print("Cluster Centers:")
print(centers)
# Calculate silhouette scores
silhouetteScores <- silhouette(kmeansWine$cluster, dist(newDf))
# Plot silhouette plot
plot(silhouetteScores, col = kmeansWine$cluster, border = NA)
# Add average silhouette width to the plot
abline(h = mean(silhouetteScores[, "sil_width"]), lty = 2)
# Average silhouette width score
avgSilhouetteWidth <- mean(silhouetteScores[, "sil_width"])
print(avgSilhouetteWidth)
# Calculate Calinski-Harabasz index using cluster.stats
calinski_harabasz <- cluster.stats(dist(newDf), kmeansWine$cluster)$ch
print("Calinski-Harabasz Index:")
print(calinski_harabasz)
# Load required libraries
library(readxl)
library(nnet)
library(neuralnet)
# Read the Excel file
df <- read_excel("ExchangeUSD.xlsx")
# Ensure that the column names are appropriate
names(df)[3] <- "USDEUR"
# Remove "YYYY/MM/DD" and "Wdy" columns
df <- df[, -c(1, 2)]
# Identify potential outliers using Tukey's method
outlierIndices <- vector("list", length = ncol(df))
for (i in seq_along(df)) {
Q <- quantile(df[[i]], c(0.25, 0.75), na.rm = TRUE)
IQR <- Q[2] - Q[1]
lowerBound <- Q[1] - 1.5 * IQR
upperBound <- Q[2] + 1.5 * IQR
outlierIndices[[i]] <- which(df[[i]] < lowerBound | df[[i]] > upperBound)
}
# Combine outlier indices
allOutlierIndices <- unique(unlist(outlierIndices))
# Remove outliers
dfNoOutliers <- df[-allOutlierIndices, ]
# Create lagged variables up to lag 4 for the exchange rate column
for (i in 1:4) {
dfNoOutliers[[paste0("Lag", i)]] <- c(rep(NA, i), head(dfNoOutliers$USDEUR, -i))
}
# Remove rows with NA values introduced by lagging
dfNoOutliers <- dfNoOutliers[complete.cases(dfNoOutliers), ]
# Rename column names
inputVars <- c("Lag1", "Lag2", "Lag3", "Lag4")  # Input variables
outputVar <- "USDEUR"  # Output variable
# Normalize the dataset using Min-Max method
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# Apply normalization to all columns except the output variable
dfFinal <- as.data.frame(lapply(dfNoOutliers[, inputVars], normalize))
# Add the output variable to the normalized dataframe
dfFinal <- cbind(dfFinal, USDEUR = dfNoOutliers$USDEUR)
# Split the dataset
trainData <- dfFinal[1:400, ]
testData <- dfFinal[401:nrow(dfFinal), ]
# Function to build MLP models with different configurations
buildMlpModel <- function(trainData, inputVars, outputVar, hiddenLayers, linearOutput, activationFunction) {
formula <- as.formula(paste(outputVar, "~", paste(inputVars, collapse = " + ")))
model <- neuralnet(formula = formula,
data = trainData,
hidden = hiddenLayers,
linear.output = linearOutput,
act.fct = activationFunction)
return(model)
}
# List of configurations
hiddenLayers <- list(c(5), c(10), c(15), c(5, 5), c(10, 10), c(15, 15), c(5, 10), c(10, 5), c(5, 6), c(10, 4), c(4, 15), c(5, 8), c(5, 2), c(3, 4), c(13, 12))
linearOutputs <- c(TRUE, FALSE)
activationFunctions <- c("logistic", "tanh")
# Build 15 MLP models with different configurations
mlpModels <- list()
counter <- 1
for (hiddenLayer in hiddenLayers) {
for (linearOutput in linearOutputs) {
for (activationFunction in activationFunctions) {
modelName <- paste("exchangeRatesModel", counter, sep = "")
mlpModels[[modelName]] <- buildMlpModel(trainData, inputVars, outputVar, hiddenLayer, linearOutput, activationFunction)
counter <- counter + 1
if (counter > 15) {
break
}
}
if (counter > 15) {
break
}
}
if (counter > 15) {
break
}
}
# Function to calculate MAPE
calculateMape <- function(actual, predicted) {
if (length(actual) == 0 | length(predicted) == 0) {
return(NA)
}
mape <- mean(abs((actual - predicted) / actual)) * 100
return(mape)
}
# Function to calculate sMAPE
calculateSmape <- function(actual, predicted) {
if (length(actual) == 0 | length(predicted) == 0) {
return(NA)
}
smape <- 2 * mean(abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
return(smape)
}
# Calculate metrics for each model
metrics <- data.frame(Model = character(), RMSE = double(), MAE = double(), MAPE = double(), sMAPE = double(), stringsAsFactors = FALSE)
for (i in 1:length(mlpModels)) {
modelName <- paste("exchangeRatesModel", i, sep = "")
model <- mlpModels[[modelName]]
# Make predictions
predicted <- predict(model, newdata = testData)
if (length(predicted) == 0) {
next
}
# Denormalize the predictions
denormalize <- function(x, originalData) {
minVal <- min(originalData)
maxVal <- max(originalData)
return(x * (maxVal - minVal) + minVal)
}
denormalizedPredictions <- denormalize(predicted, dfNoOutliers$USDEUR)
# Calculate RMSE
rmse <- sqrt(mean((testData$USDEUR - denormalizedPredictions)^2))
# Calculate MAE
mae <- mean(abs(testData$USDEUR - denormalizedPredictions))
# Calculate MAPE
mape <- calculateMape(testData$USDEUR, denormalizedPredictions)
# Calculate sMAPE
smape <- calculateSmape(testData$USDEUR, denormalizedPredictions)
# Store metrics
metrics[i, ] <- list(modelName, rmse, mae, mape, smape)
}
# View metrics
print(metrics)
# Plot actual vs predicted exchange rates for model 3
model3 <- mlpModels[["exchangeRatesModel3"]]
predictedModel3 <- predict(model3, newdata = testData)
denormalizedPredictionsModel3 <- denormalize(predictedModel3, dfNoOutliers$USDEUR)
plot(testData$USDEUR, denormalizedPredictionsModel3,
main = "Actual vs Predicted Exchange Rates - Model 3",
xlab = "Actual Exchange Rates",
ylab = "Predicted Exchange Rates",
col = "blue",
pch = 20)
abline(0, 1, col = "red")
# Plot actual vs predicted exchange rates for model 4
model4 <- mlpModels[["exchangeRatesModel4"]]
predictedModel4 <- predict(model4, newdata = testData)
denormalizedPredictionsModel4 <- denormalize(predictedModel4, dfNoOutliers$USDEUR)
plot(testData$USDEUR, denormalizedPredictionsModel4,
main = "Actual vs Predicted Exchange Rates - Model 4",
xlab = "Actual Exchange Rates",
ylab = "Predicted Exchange Rates",
col = "blue",
pch = 20)
abline(0, 1, col = "red")
